%!TEX root = paper.tex


\section{Introduction}
\label{sec:intro}
Data stream processing (e.g, as exemplified by Complex Event Processing systems) and static (batch) data processing (e.g., as exemplified by MPP databases and Hadoop ) were traditionally considered as two very different types of applications. They were programmed using different programming models and APIs, and were executed by different systems (for example, dedicated streaming systems such as  Apache Storm, IBM Infosphere Streams, Microsoft Streaminsight, or Streambase versus relational databases or execution engines for Hadoop, including Apache Spark, Apache Drill, etc). Traditionally, batch data analysis made up for the lion share of the use cases, data sizes, and market, while streaming data analysis mostly served specialized applications.

It is becoming more and more apparent, however, that a huge number of today's large scale data processing use cases handle data that is, in reality, continuously produced over time. These continuous streams of data come for example from web logs, application logs, sensors, or as changes to application state in databases (transaction log records). Rather than treating the streams as streams, today's setups ignore the continuous and timely nature of data production. Instead, data records are (often artificially) batched into static data sets (for example hourly/daily/monthly chunks) and then processed in a time-agnostic fashion. Data collection tools, workflow managers, and schedulers orchestrate the creation and processing of batches, in what is actually a continuous data processing pipeline. Architectural patterns like the "lambda architecture" \cite{marz2015big} combine batch and stream processing systems to implement multiple paths of computation: A streaming fast path for timely approximate results, and a batch offline path for late accurate results. All these approaches suffer from high latency (imposed by batches), high complexity (connecting and orchestrating several systems, and implementing business logic twice), as well as arbitrary inaccuracy, as the time dimension is not explicitly handled by the application code.

Apache Flink follows a paradigm that embraces data stream processing as the unifying model for real-time analysis, continuous streams, and batch processing, both in the programming model and in the execution engine. In combination with durable message queues that allow quasi arbitrary replay of data streams (like Apache Kafka\footnote{http://kafka.apache.org/} or Amazon Kinesis\footnote{https://aws.amazon.com/kinesis/}), stream processing programs make no difference between processing the latest events in real-time, continuously aggregating data periodically in large windows, or processing terabytes of historical data: these types of computations simply start their processing at different points in the durable stream, and maintain different forms of state during the computation. Through a highly flexible windowing mechanism, Flink programs can compute both early and approximate, as well as delayed and accurate results in the same operation, obviating the need to combine different systems for the two use cases. Flink supports different notions of time (event time, ingestion time, processing time) in order to give programmer high flexibility in defining how events should be correlated.
 
At the same time, Flink acknowledges that there is, and will be, a need for dedicated batch processing (dealing with static data sets). Complex queries over static data are still a good match for a batch processing abstraction. Furthermore, batch processing is still needed both for legacy implementations of streaming use cases, and for analysis applications where no efficient algorithms are yet known that perform this kind of processing on streaming data. Batch programs are special cases of streaming programs, where the stream is finite, and order/time of records does not matter (all records implicitly belong to one all-encompassing window). However, to support batch use cases with a competitive ease and performance, Flink has a specialized API for processing static data sets, uses specialized data structures and algorithms for the batch versions of operators like join or grouping, and uses dedicated scheduling strategies. The result is that Flink presents itself as a full-fledged and efficient batch processor on top of a streaming runtime, including libraries for graph analysis and machine learning. 
Originating from the Stratosphere project \cite{stratosphere}, Flink is a top-level project of the Apache Software Foundation that is developed and supported by a large and lively community (of more than 140 open source contributors at the time of this writing\footnote{For an update list, refer to \url{https://github.com/apache/flink/graphs/contributors}}), and is used in production in several companies.

\vspace{2mm}
\noindent The contributions of this paper are the following:\vspace{-2mm}
\begin{itemize}
	\item We make the case for a unified architecture of stream and batch data processing, including specific optimizations that are only relevant for static data sets.
	\vspace{-3mm}
	\item We show how streaming, batch, iterative, and interactive analytics can be represented as fault tolerant streaming dataflows (\autoref{sec:execution}).
	\vspace{-3mm}
	\item We discuss how we can build a full-fledged stream analytics system with a flexible windowing mechanism (\autoref{sec:streaming}), as well as a full-fledged batch processor (\autoref{sec:batch}) on top of these dataflows, by showing how streaming, batch, iterative, and interactive analytics can be represented as streaming dataflows.
\end{itemize}