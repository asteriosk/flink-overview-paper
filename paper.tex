\documentclass{sig-alternate}

\usepackage{deauthor,times,graphicx}



\begin{document}



\title{Apache Flink: Stream and Batch Processing at Scale}
\author{Authors}

\maketitle

\begin{abstract}
Abstract TBD.
\end{abstract}

Apache Flink™ : Streaming dataflows as a basis for universal data analytics

Abstract

Motivation
Organizations rely more and more on the value of their data, which is directly correlated with the freshness of the data. Additionally, as the data infrastructure becomes more and more complex, development time increasingly goes into plumbing and integrating systems together rather than focusing on the business problem. These trends, together with the ever-growing need for lower latency (“real-time”), and scale create a pressing need for more real-time, asynchronous, push-based data infrastructure. This kind of infrastructure “turns the database around”, by allowing isolated applications to interact with their local datasets, rather than imposing the need to coordinate on a global data store. In conjunction with the growing trends of cloud-based data processing, containerization, Internet of Things, and microservices creates a new reality for the data infrastructure. Popularized by internet companies such as LinkedIn, this trend is quickly sweeping through the enterprise.

Collectively referred to as streaming, this new kind of infrastructure is based around the following notions. Data items (records) represent real-world events (e.g., a tap on a smartphone, a sensor reading, or a database transaction). An event stream is a time-ordered series of records together with the timestamp that they “happened”, measured, e.g., by the smartphone itself. An event stream can be thought of as a log, in which new events can only be added at the tail, and events can only be consumed from the head. To process historical data, the log needs to be replayed from a past logical time. Applications can consume event streams and create and maintain their own views. Contrast this paradigm with the traditional database-middleware paradigm. With streaming there is no global consistent view of the world that applications need to interact with. Rather, the history of the world is the ground truth, and isolated applications maintain their own views, often referred to as state. 

Of course, none of this is really new. A large part of the enterprise infrastructure has always been based on continuously flowing event streams, typically served by tools such as enterprise message buses (e.g., TIBCO), CEP (Complex Event Processing) systems (e.g., IBM Infosphere Streams, Microsoft Streaminsight, Streambase), or roll-your-own internal systems. CEP systems in particular are based on years of research in the database community with projects such as Aurora, Telegraph, STREAMS, [[Microsoft project]], etc. While these systems had always served a niche market well, they never achieved widespread adoption in the industry for a variety of reasons, including limited scalability, cost, and lack of integration with other parts of the data infrastructure. Another reason for the limited adoption is, of course, timing. The “Big Data” movement has created an explosion of data, most of which is of a continuous, unbounded nature.

Another way that industry is tackling this problem is by using a traditional batch processing paradigm to serve streaming use cases. This has led to approaches such as “micro-batching” [], where a batch processor is used to simulate continuous processing, and the “lambda architecture”, where a stream processor is aided by a batch processor to compensate for lack of performance and consistency guarantees.

To summarize, there is a pressing need for streaming infrastructure that scales in order to tackle both integration and latency. The historical lack of good implementations of such systems has led to one-off solutions, or bolt-on solutions with questionable performance. What modern enterprises need is a streaming platform that is (1) consistent, (2) general, (3) efficient, and (4) scalable. 

Apache Flink is a platform that delivers on these requirements. Flink is an open source project licensed by the Apache Software Foundation (ASF), a non-profit organization that houses open source projects and their communities. The Flink community is counting more than 120 developers, and growing fast []. Historically, Flink stems from the database research community, and in particular the Stratosphere research project []. Flink entered the ASF in April 2014 as an “incubating” project, and “graduated” as a “top-level” project in December 2014. 

Flink incorporates several novel research contributions, some of which have been published in the past, and some of which have only been published in the form of blog posts or wiki pages. In this article, we attempt a complete description of all novel aspects of the system, including:
A novel paradigm for distributed dataflows based on the notions of operators and logical intermediate results.
A novel mechanism to provide fault tolerance in a distributed data streaming system while keeping the latency low and the throughput high.
How to overcome limitations of the JVM by careful engineering
Efficiently embedding iterations within a DAG-based dataflow system [vldb12]
Extending cost-based optimization to user-defined functions [socc10, vldb12]
The value of this paper lies mostly into positioning all these within a common architecture and few conceptual building blocks. We believe that Flink is the artifact of “Big Data systems done right”, clearly superseding most other proposed architectures in the space in all fronts (including batch, stream, iterative processing).
System architecture
We begin by laying out the architecture of Flink (i) as a software stack, and (ii) as a distributed system.

While Flink’s stack of APIs continues to grow, we can distinguish four main layers: deployment, core, APIs, and libraries.



The core of Flink is the distributed dataflow engine, which executes dataflow programs called JobGraphs. A JobGraph is a DAG of operators and connections between operators, but it is oblivious to the semantics of these operators: for example, the engine does not distinguish between a join or a coGroup, and it similarly does not distinguish between a map or a stream window operator; semantics are left to the upper layers, which embed the specific operator logic withing the boxes provided by the runtime engine.

There are two “core” APIs in Flink: the DataSet API for processing finite data sets (often referred to as “batch processing”), and the DataStream API for processing potentially unbounded data sets (often referred to as “stream processing”). We note that the the terms “batch” and “stream processing” here only cause confusion, as the carry the assumption that the underlying engine uses a particular style of physical data processing [Dataflow, Streaming 101]. In Flink, the core runtime engine can be thought of as a streaming dataflow engine, and both APIs create programs (JobGraphs) accepted by this engine. The reason for the existence of the two APIs is largely historical. The DataSet API pre-existed to the DataStream API. In theory, one can treat finite data sets as a special case of unbounded data streams, and embed all functionality of the DataSet API within the DataStream API. 
On top of the core APIs, Flink bundles “domain-specific” libraries and APIs, currently:
Gelly, an API and library for processing graphs
FlinkML, an API and library for composing Machine Learning pipelines
Table, an API similar in spirit to Microsoft’s LINQ
All these APIs generate DataSet and DataStream API programs. In addition, Flink includes several compatibility packages, currently a Hadoop MapReduce and an Apache Storm compatibility package. Finally, third-party tools can also generate Flink programs. These are not shipped with the Flink codebase, but are part of other software projects. A non-exhaustive list of these are Cascading [], Google Cloud Dataflow [], Apache SAMOA [], Apache Mahout [], and Apache Zeppelin [].

A Flink cluster involves three main types of processes: the client, the JobManager, and the TaskManager. When Flink is running in a YARN cluster, the JobManager and TaskManager processes are embedded within YARN containers. A typical cluster consists of one client (typically living in the machine that submits the program), one or more JobManagers (one leader and one or more replicas for high availability), and several TaskManagers, typically one per cluster node. 

[[Explain program compilation in detail, what JobManager does and main JM data structures, what TaskManagers do, and main TaM data structures, how messages are shipped, how data is transferred, and how heartbeats and high availability work]].
The core: continuous dataflows 

Programmers interact with Flink using a combination of its APIs (currently DataSet, DataStream, Table, Graph, and Machine Learning). Every Flink program goes through a compilation phase (a combination of type inference, query optimization, and code generation, all described in the subsequent sections), and creates a dataflow graph (called JobGraph in Flink parlance) that serves as a unifying representation of all Flink programs.

A JobGraph is a directed acyclic graph (DAG) that consists of nodes and edges. There are two classes of nodes: (stateful) operators, and (logical) intermediate results (IRs). For example, the following graph consists of five operators (o1-o5), and three intermediate results (ir1-ir3). Operators abstract computation (e.g., transformations, joins, etc), state (e.g., a persistent counter), as well as data sources (e.g., reading data from a file system, a socket, a message queue, etc), and data sinks. Operators produce intermediate results, as well as updates to state (which can be managed externally). An intermediate result is a logical handle (pointer) to the data that is produced by one operator. An intermediate result can be consumed by one or more operators. Intermediate results are logical in the sense that the data they point to may or may not be materialized on disk.



When the JobGraph is scheduled in a cluster for execution (Flink can run on standalone and YARN clusters), it is parallelized to form an ExecutionGraph. An ExecutionGraph consists of tasks (parallel instances of operators), and intermediate result partitions (IRPs), that represent logical partitions of intermediate results corresponding to different partitioning keys. Let us take the above JobGraph and partition it using a parallelism of 2. Assume that o1 and o2 are data sources, o2 is a map operator, o4 is a join operator that needs both inputs partitioned by the same key, and o5 is a filter. Assume also that the join is executed as a repartitioned join, i.e., both inputs are partitioned and shuffled over the cluster of 2 nodes (alternatives would be to broadcast one input or leverage a pre-existing partitioning if present). The resulting ExecutionGraph is the following:



The unit of data transfer in the Flink runtime is a buffer. Buffers contain one or more records, and a record can span multiple buffers. The default size of a buffer in Flink is 32 kilobytes. Buffers are requested and relinquished from local buffer pools, shared among operators that live in the same task manager. An IRP is simply a collection of buffers. Work in Flink progresses (i.e., records flow through the pipeline) as long as there are buffers available, essentially implementing distributed blocking queues (the logical streams) with bounded capacity (the amount of memory available to the buffer pools, which can be configured by the user). Flink  This mechanism, in addition to implementing record network transfers doubles down as a natural way to backpressure the flow in the case of slow operators (including external systems that consume data). See [http://data-artisans.com/how-flink-handles-backpressure/] for a detailed presentation. 

We mentioned that the intermediate results are logical handles to the data, rather than the data itself. Internally, and intermediate result is an abstract class that has many implementations. Let us look at two concrete examples: implementing pipelined data exchange, and implementing blocking data exchange.

Pipelined data exchange (also called intra-operator parallelism) means that a producing and a consuming operator make progress at the same time, without the consumer waiting for the producer to finish. Pipelining is present (and required) in streaming systems (such as Flink, Storm, CEP systems, etc), and is also used in MPP databases and DBMSs (Impala, Greenplum), but it is typically not used in batch processors (Spark, MapReduce, Tez). Pipelined data exchange is implemented in Flink by an intermediate result implementation, which activates network transfers as soon as the first buffer is available. In particular, as soon as the producing operator places the first buffer in the cache of the result partition, the IR signals to the JobManager its availability. The JobManager then signals to all consumers of this partition that the the partition is ready (scheduling the operators if they had not been previously scheduled), and initiates the data transfer.

Pipelined data exchange is great for lower latency (time to first result) in batch jobs, and is required in streaming jobs. To better allocate resources in very large batch jobs, it is sometimes desirable to break up a job into stages, scheduling and executing each stage individually. To do that, a blocking data exchange that materializes intermediate results in a combination of memory and disk is desirable. Flink implements blocking data exchange via an intermediate result that signals its availability only when all the buffers from the producer have been cached. The cached buffers double down as a materialized reusable intermediate result, similar to Spark’s RDD data structure. Blocking data exchange is used in Flink to break deadlocks, or when specified by the user. All other data processing needs (including batch processing) currently use pipelined data exchange.

All available implementations of intermediate results in Flink are shown in the following table:



Pipelined
Materialized
Ephemeral




Cached






See [https://cwiki.apache.org/confluence/display/FLINK/Data+exchange+between+tasks] for a detailed presentation.

Fault tolerance via asynchronous snapshots
Backing up and restoring the state of a computation is vital in the presence of software or infrastructure failures. On the event of such a failure, the system has to recover the state of the failed operator, and continue the computation from where it left off. As in transaction execution, different applications require different consistency guarantees. For instance, if no consistency guarantee is required (e.g., a live monitoring dashboard that displays clicks in a website), a system might simply recover the operator state to any previous snapshot and continue ingesting the stream. However, a computation that requires correctness of results (e.g., user interactions with their shopping basket) requires stricter consistency guarantees.

Such consistency guarantees include:
at most once: no consistency guarantees, including possibility for data loss 
at least once: the fault tolerance mechanism guarantees that an event is going to touch the state of an operator at least once i.e., in case of failure, some events of the stream might touch the state multiple times.
exactly once: that the operator state will be touched exactly once per event, no matter how many failures may happen.

In batch processing, guaranteeing exactly once consistency is simple: the system simply pauses the computation, re-deploys (part of) the dataflow and re-consumes the necessary parts of its input. In the streaming case, however, i) computations are continuous (e.g., months of processing) ii) a data stream does not have, in principle, a beginning or an end, nor can it be stored indefinitely and iii) the computation cannot simply be stopped mid-flight.
Exactly-Once Fault Tolerance in Flink
The problem of providing exactly once guarantees boils down to determining what state the streaming computation currently is in (including in-flight records, and operator state), drawing a consistent snapshot of that state, and storing that snapshot in durable storage. If one can do this frequently, recovery from a failure only means i) restoring the latest snapshot from durable storage, and ii) rewinding the stream source to the point when the snapshot was taken and continuing the computation. Flink’s algorithm is described in detail in []; in the sequel, we give a brief summary.

Flink’s snapshot mechanism is based on an improvement of a technique introduced in [Chandy/Lamport], to draw consistent snapshots of the current state of a distributed system without missing information and without recording duplicates. In this technique, the snapshots are taken asynchronously: the computation does not need to stop in order to take a snapshot. Flink’s fault tolerance mechanism draws state snapshots of a running stream topology, and stores these snapshots to durable storage (e.g., to HDFS or an in-memory file system). The frequency of these checkpoints can vary according to the failure rates and the cost of taking a snapshot.

The checkpointing mechanism is based on stream barriers (“markers” in []) that flow through the operators. Barriers are first injected at the sources (e.g., if using Apache Kafka as a source, barriers are aligned with offsets), and flow through the DAG as part of the data stream together with the data records (see figure below). A barrier separates records into two groups: those that are part of the current snapshot (a barrier signals the start of a checkpoint), and those that are part of the next snapshot.



Barriers flow downstream and trigger state snapshots when they pass through operators. An operator first aligns its barriers from all incoming streams (if the operator has more than one input), buffering data from the faster ones. When an operator has received a barrier from all incoming streams, it checkpoints its state (if any) to durable storage. Once state checkpointing is done, the operator forwards the barrier downstream. Note that in this mechanism, state checkpointing can be both asynchronous (processing continues while the state is written out to durable storage) and incremental (only deltas are written to durable storage). Once all data sinks have released the barriers, the current checkpoint has finished. Recovery on a failure means simply restoring the latest checkpointed state, and restarting the sources from the last recorded barrier.

The aforementioned mechanism guarantees “exactly-once” state updates, meaning that any state that takes part in the snapshotting mechanism will have the same value with or without failures. Flink also includes a mode for at-most-once processing (no snapshotting, no barriers), as well as a mode for at least once processing (operators simply forward the barriers, but sources snapshot their offsets). 

A common misinterpretation of the term “exactly-once” is that the output of a streaming job does not have any duplicates. That might still be the case, as data sinks are external systems, which Flink might not have access to. However, by building custom connectors to common systems that participate in Flink’s snapshotting  mechanism, one can ensure end-to-end exactly once guarantees. Flink bundles a number of these connectors. For example, Flink bundles a data sink to any HDFS-compatible file system that creates rolling files from a continuous streaming job. The user can specify when a new file will be created. In Flink data sinks are stateful operators just as any other operator. The state of the HDFS sink is the last offset written in the file. In the case of a failure, restoring the sink to its snapshotted state triggers a file truncation that deletes the data that was written between the last snapshot and the failure. 

Thus, Flink can be seen as a way to implement distributed transactions between systems. By creating sources and sinks that take part in the snapshotting, and relaying the data movement to Flink, one can move data consistently with transactional guarantees between systems. For example, the HDFS sink describes above implements “read uncommitted” semantics, as “garbage” data may be written and then deleted. We note that such systems implemented with Flink are CA systems in the CAP theorem characterization. 
Stream analytics on top of dataflows
Window implementation
Batch analytics on top of dataflows
How to embed blocking operators in dataflow graph
Why this is not enough
Blocking data exchange (stages)
Scheduling (WiP)
Fault tolerance (WiP)
Query optimization
Memory management

How Flink implements those
Query optimization
Motivation?

Flink’s optimizer builds on technology from parallel database systems; namely, on the concepts of plan equivalences, cost models and interesting properties. Yet, certain issues which  stem from the UDF-heavy DAGs that represent Flink’s dataflow programs, do not allow Flink’s optimizer to employ those database techniques out of the box [black-boxes]. In the sequel, we summarize these issues and present the solutions that Flink implements.

The first issue is that queries in most database systems are represented and executed as trees of operators (i.e., each operator has exactly one output). whose semantics are well defined  and known to the optimizer. Based on those semantics, traditional optimizers enumerate alternative logical plans by e.g., employing equivalence rules for operator reordering [volcano, cascades]. In contrast, Flink’s programs do not form trees of known operators but DAGs, which encapsulate UDFs with arbitrary user code. As a result, the operators hide their semantics from the optimizer and traditional techniques for plan enumerator are not directly applicable. Worse, traditional cardinality and cost estimation methods do not apply either, due to the presence of UDFs. Flink’s optimizer employs a number of novel methods for overcoming these issues [blackboxes, stratosphere-journal, iterations] for which we provide an short overview below.
Plan Enumeration for UDF-heavy DAGs
Plan enumeration (reordering) in the presence of UDFs. Semantics of operators. etc.

Physical Optimization
Flink’s runtime supports execution strategies known from parallel databases, such as repartition/broadcast data transfer strategies, as well as sort-based grouping and different join implementations (sort- and hash-based). Flink’s optimizer enumerates different physical plans based on the concept of interesting properties propagation [scope-optimizer]. 

The optimizer uses a cost-based approach to choose among multiple equivalent physical plans, based on network/disk I/O, and CPU cost. For the optimizer to assign costs to plans, it needs to first estimate the size of intermediate results. Estimating the cardinality of UDFs’ outputs is very challenging, even in relational databases [], and physical properties can be destroyed by a UDF (e.g., by modifying the field on which the input of an operator is sorted). To overcome these issues,  Flink’s optimizer relies on hints that are provided by the programmer. Examples of hints can be to state which side of a join is very small (e.g., could lead to the execution of a broadcast join) or that a certain field of a record, entering a UDF, is forwarded intact (i.e., physical properties such as sorting on that field are preserved and can be propagated to further parts of the plan). These hints enrich the internal DAG representation of a Flink program with extra information and allow the optimizer to perform propagation of interesting physical properties in the DAG. 

Previous work on constructing optimal plans for DAG-structured queries have shown that it is a very challenging problem [neumann-moerkete].  Flink performs the propagation of interesting properties (sorting, partitioning, etc.) from the sources to the sinks and then performs a pass from the sinks to the sources in order to prune the possible plans and choose the optimal one, similarly to [scope-optimizer]. To correctly enumerate DAG-structured plans, Flink first analyses the DAG in order to find the operators that “branch” (i.e., have more than one outputs) and the operators that “join” those branches back together. The subplans rooted on the branching operator are treated like common subexpressions;  The plan candidates for that operator’s inputs must have the same subplan for the common subexpression. Finally, the optimizer adds artificial pipeline breakers to plans with deadlocks. A deadlock can happen when some of the paths between the branching and the joining operator are fully pipelined (i.e., pipelined operators, depend - possibly indirectly -  on each other).

Iteration-specific optimizations here or in the iterations section?
Memory Management
In data analysis frameworks such as Flink, data needs to be stored in memory, for data caching and executing operations such as sorting and joining. A straightforward approach of processing in a JVM, is to store the data as Java objects, and operate on those, leaving the responsibility of garbage collection and spilling to disk to the JVM and the OS respectively. In that case, the overhead of garbage collection in a JVM heap filled with millions of objects, can easily exceed the actual computation time. Moreover, Java objects require a certain space overhead, depending on the JVM implementation and platform. Database operator implementations have been traditionally managing memory and employing smart memory spilling mechanisms [].



Building on database technology, Flink, instead of storing objects in the heap, serializes objects into a fixed number of pre-allocated memory segments. Figure X, depicts the structure of a memory segment and the internal representation of a tuple of 3 fields: an integer, a double and a nested Pojo (a Person that has an ID and a name).

In Flink, operations such as sorting, and joining operate as much as possible on the binary data, keeping the de/serialization overhead at a minimum. If more data needs to be processed than can be kept in memory, Flink’s operators partially spill data to disk. Flink builds on, and generalizes database technology to handle arbitrary objects using type inference, and  custom serialization and comparison mechanisms. 

By employing such techniques for data processing off-heap and in binary representation, Flink manages to, not only reduce the garbage collection overhead, but also to perform memory-safe execution and implement cache-efficient and robust out-of-core algorithms.
Iterative analytics on top of dataflows
Supersteps or asynchronous
Head and tail
Delta iterations
Backup of in-flight records in snapshotting

Related work
Fault Tolerance in Streaming Systems
In stream processing, fault tolerance with exactly-once guarantees implemented in many different ways. For instance, Google Dataflow/Millwheel does it by storing, in a transactional store, the in-flight records (the ones that it pushes downstream) together with the current state, while in systems that implement micro-batching (e.g., Spark, Trident), the resulting state at each micro-batch step is checkpointed (as described above for the batch case) and can be easily recovered in case of failures.



Conclusion

References




\cite{DBLP:conf/hotcloud/ZahariaCFSS10}


\bibliographystyle{abbrv}
\bibliography{references}

\end{document}