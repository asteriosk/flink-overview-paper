%!TEX root = paper.tex

\section{Batch analytics on top of dataflows}
\label{sec:batch}
A bounded data set is a special case of an unbounded data stream. Thus, a streaming program that inserts all of its input data in a window can form a ``batch`` program and ``batch analytics``, or ``batch processing`` should be fully covered by Flink's features that we presented above. However, \begin{inparaenum}[i)]
  \item the syntax, i.e., the API for batch computation can be simplified (not need for window definitions, simpler joins and loops),
  \item we can simplify the fault tolerance mechanisms and
  \item we can apply query optimization borrowing ideas from MPP database systems.
\end{inparaenum}
For these reasons, Flink treats specially batch computations, by implementing the above optimizations. For enabling batch computations on top of it's streaming runtime, Flink embeds blocking versions of its operators (sorts, joins, etc) within its runtime. These operators simply block until they have received all of their input. Moreover, Flink currently disables the asynchronous snapshotting mechanism for batch programs, and simply uses backtracking based recovery as first described in Dryad \cite{isard2007dryad}. In the sequel, we look that the batch-specific optimizations mentioned above: query optimization, and query processing on paged (managed) memory.

\para{Query Optimization.} Flink's optimizer builds on techniques from parallel database systems such as plan equivalences, cost models and interesting properties. However, the arbitrary UDF-heavy DAGs that make up Flink's dataflow programs, do not allow a traditional optimizer to employ database techniques out of the box \cite{blackBoxes}, since the operators hide their semantics from the optimizer. For the same reason, cardinality and cost estimation methods are equally difficult to employ. Flink's optimizer employs a number of novel methods for overcoming these issues~\cite{blackBoxes, stratosphere, DBLP:journals/pvldb/EwenTKM12} for which we provide a short overview below. Flink's runtime supports various execution strategies including repartition/broadcast data transfer, as well as sort-based grouping and sort- and hash-based join implementations. Flink's optimizer enumerates different physical plans based on the concept of interesting properties propagation~\cite{scopeOptimizer}, using a cost-based approach to choose among multiple physical plans. The cost includes network/disk I/O and CPU cost. To overcome the cardinality estimation issues that were mentioned earlier, Flink's optimizer uses hints that are provided by the programmer.

\para{Memory Management.} Building on database technology, Flink, instead of storing objects in the JVM's heap, serializes objects into a  memory segments. These memory segments resemble database blocks into which, Java objects representing the tuples that go through the runtime are serialized. Operations such as sorting, and joining, operate as much as possible on the binary data, keeping the de/serialization overhead at a minimum and partially spilling data to disk when needed. To handle arbitrary objects, Flink uses type inference, and  custom serialization mechanisms.  By keeping the data processing on binary representation and off-heap, Flink manages to reduce the garbage collection overhead, and implement cache-efficient and robust algorithms that scale gracefully in under memory constraints.

\para{Native Iterations on top of Dataflows.} The final aspect of Flink on which we focus, is how to implement loops on top of the Flink's dataflow engine. Some approaches execute iterations by submitting new jobs for each iteration or by adding additional nodes to a running DAG \cite{DBLP:journals/pvldb/BuHBE10, DBLP:conf/hotcloud/ZahariaCFSS10}, hiding from  the engine that it is executing an iterative program. The approach, implemented in Naiad~\cite{murray2013naiad} adds feedback edges in the dataflow graph, supporting graphs with cycles, that  allow for nested iterations. A third approach was to design specialized engines around iterative processing along (e.g., Apache Giraph, and GraphLab) allowing to reduce the number of computations in each iteration \cite{low2012distributed}.

Flink follows an approach that maintains the DAG-based runtime, but allows for special ``head'' and ``tail'' tasks to signify the beginning and end of iteration, that exchange data via shared memory. This approach simulates a cyclic dataflow within a DAG engine making Flink competitive with specialized graph engines \cite{DBLP:journals/pvldb/EwenTKM12}, while outperforming the driver-based approach. To do that, Flink supports delta iterations, which exploit sparse computational dependencies, and are used, as the basis for Gelly, Flink's Graph API. Finally, Flink offers Bulk Synchronous Parallel (BSP) iterations in its DataSet API, and asynchronous iterations in its DataStream API.
