%!TEX root = paper.tex

\section{Batch analytics on top of dataflows}
\label{sec:batch}



A bounded data set is a special case of an unbounded data stream. Thus, a streaming program that inserts all of its input data in a window can form a “batch” program and “batch processing” should be fully covered by Flink’s features that we presented above. However, i) the syntax, i.e., the API for batch computation can be simplified (e.g., there is not need for artificial global window definitions, simpler join syntax), and iii) programs that process bounded data sets are amenable to additional optimizations, more efficient book keeping for fault tolerance, and staged scheduling. .

\vspace{2mm}
\noindent Flink approaches batch processing as follows:
\begin{itemize}
	\item Batch computations are executed by the same runtime as streaming computations. The runtime executable may be parameterized with blocked data streams to break up large computations in isolated stages that are scheduled successively.
	\item Periodic snapshotting is turned off. Instead, fault recovery is achieved by replaying the lost stream partitions from the latest buffered stream (possibly the source).
	\item Blocking operators (e.g., joins and sorts) are simply operator implementations that happen to block until they have consumed their entire input. The runtime is not aware of whether an operator is blocking or not. These operators use managed memory provided by Flink (either on- or off- the JVM heap), and can spill to disk if their inputs exceed their memory bounds.
	\item A dedicated DataSet API provides familiar abstractions for batch computations, namely a bounded fault-tolerant DataSet data structure, and transformations on DataSets (e.g., joins, aggregations, iterations, etc)
	\item A query optimization layer transforms a DataSet program to an efficient executable.
\end{itemize}



\para{Query Optimization.} Flink’s optimizer builds on techniques from parallel database systems such as plan equivalence, cost modeling and interesting property propagation. However, the arbitrary UDF-heavy DAGs that make up Flink’s dataflow programs do not allow a traditional optimizer to employ database techniques out of the box \cite{blackBoxes}, since the operators hide their semantics from the optimizer. For the same reason, cardinality and cost estimation methods are equally difficult to employ. Flink’s runtime supports various execution strategies including repartition/broadcast data transfer, as well as sort-based grouping and sort- and hash-based join implementations. Flink’s optimizer enumerates different physical plans based on the concept of interesting properties propagation \cite{scopeOptimizer}, using a cost-based approach to choose among multiple physical plans. The cost includes network/disk I/O and CPU cost. To overcome the cardinality estimation issues that were mentioned earlier,  Flink’s optimizer uses hints that are provided by the programmer.


\para{Memory Management.} Building on database technology, Flink, instead of allocating objects in the JVM’s heap to represent buffered in-flight data records, serializes data into a  memory segments. Operations such as sorting and joining operate as much as possible on the binary data directly, keeping the de/serialization overhead at a minimum and partially spilling data to disk when needed. To handle arbitrary objects, Flink uses type inference, and  custom serialization mechanisms.  By keeping the data processing on binary representation and off-heap, Flink manages to reduce the garbage collection overhead, and use cache-efficient and robust algorithms [Graefe join paper] that scale gracefully under memory pressure.

\para{Native Iterations on top of Dataflows.}  Earlier dataflow approaches execute iterations by submitting new jobs for each iteration or by adding additional nodes to a running DAG \cite{DBLP:journals/pvldb/BuHBE10, DBLP:conf/hotcloud/ZahariaCFSS10}, hiding from  the engine that it is executing an iterative program. The approach, implemented in Naiad \cite{murray2013naiad} adds feedback edges in the dataflow graph, supporting graphs with cycles, that  allow for nested iterations. A third approach was to design specialized engines around iterative processing along (e.g., Giraph, and GraphLab) allowing to reduce the number of computations in each iteration \cite{low2012distributed}.

Flink follows an approach that retains the DAG-based runtime, but allows for special “head” and “tail” tasks to signify the beginning and end of iteration, that exchange data via shared memory. This approach simulates a cyclic dataflow within a DAG engine making Flink i ) competitive with specialized graph engines \cite{DBLP:journals/pvldb/EwenTKM12}, and outperforms the driver-based approach. Flink supports delta iterations, which exploit sparse computational dependencies, and are used, as the basis for Gelly, Flink’s Graph API. Finally, Flink offers Bulk Synchronous Parallel (BSP) iterations in its DataSet API, and asynchronous iterations in its DataStream API.
